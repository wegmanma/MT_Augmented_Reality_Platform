%!TEX root = ../doc.tex
\chapter{Fundamentals}
\label{sec:Fundamentals}
The following Chapter describes methods and technologies used within this thesis.
\section{3D Cameras}
\label{sec:ToFCamera}
In 3D mapping, two expressions often get mentioned: Light-Detection and Ranging (LiDaR) sensors and Time of Flight (ToF) cameras. As the basic principle in both technologies relies on measuring the Time of Flight and is in both cases Light-Detection and Ranging, both expressions are technically ambivalent.\\
A LiDaR sensor is often referred to work together with a moving laser that scans its surroundings.\cite{Techradar_Lidar} The mechanical mounting of such a device is too bulky to be embedded in a modern smartphone, which is why solid-state LiDaR sensors are used. A solid-state LiDaR sensor projects a grid of laser dots onto the scene, as seen in Figure \ref{im:iPadLidar}. The time of flight for each dot is measured individually.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/ifixit_lidar.png}
    \caption{Projected dots from the LiDaR scanner of an Apple iPad Pro 2020, made visible with an infrared camera. Image source: iFixit.com}
    \label{im:iPadLidar}
\end{figure}
Another method for 3D mapping is using one wide-area infrared flash and measuring the time of flight on each pixel of a camera sensor. This approach, in contrast to the laser based LiDaR scanner, is often referred to be a ToF camera. Android-powered smartphones of various manufacturers use ToF cameras to improve autofocus capabilities or add artificial bokeh.\\
While the physical principle in both technologies is the same, the LiDaR scanner generates a point cloud, while the ToF camera outputs a depth map image. With mathematical transformations, both outputs are equivalent. A ToF camera also works as an infrared grayscale camera, providing an image by itself.  \\
The sensor used for this thesis follows the principle of a ToF camera with a wide area infrared flash. The measured radial distance from the sensor for each pixel allows the three-dimensional reconstruction of the scene. As the distance measurement is radial, it needs to be corrected to obtain flat surfaces. A reference measurement helps solve this problem, Section \ref{sec:ToFCalibration} explains the performed correction.

\section{Mathematics for Rotation and Translation}
\label{sec:LinAlgRotation}
Augmented reality relies on having accurate positional and angular information to estimate the required size and warp of a virtual object projected into the real world. A microelectromechanical system (MEMS) containing a gyroscope and an accelerometer provides rotation and acceleration information to the system to assist the positional tracking.
\subsection{Euler Rotations and Linear Algebra}
A common way to calculate rotations and translations are matrix-vector multiplications. The standard matrices for rotating with the angle $\phi$ around $X$-, $Y$- and $Z$-axis are 
\begin{equation*}
    A_{rot,X} =
    \begin{bmatrix}
        1 & 0        & 0         \\
        0 & cos \phi & -sin \phi \\
        0 & sin \phi & cos \phi
    \end{bmatrix}
    ,\quad
    A_{rot,Y} =
    \begin{bmatrix}
        cos \phi  & 0 & sin \phi \\
        0         & 1 & 0        \\
        -sin \phi & 0 & cos \phi
    \end{bmatrix}
    ,\quad
    A_{rot,Z} =
    \begin{bmatrix}
        cos \phi  & sin \phi & 0 \\
        -sin \phi & cos \phi & 0 \\
        0         & 0        & 1
    \end{bmatrix}.
\end{equation*}
A combination of the three matrices leads to a rotation matrix with a rotation axis that is not strictly bound to $X$, $Y$, or $Z$. Matrix multiplication is not commutative, so the order of the multiplications matters. In the following example, the vector gets rotated first around $X$, then $Y$, and around $Z$ in the end. A chain of rotations around $X$,  $Y$, and $Z$ results in a single rotation around an arbitrary axis for a specific angle using the matrix
\begin{equation*}
    A_{rot}
    =A_{rot,Z} \cdot A_{rot,Y} \cdot A_{rot,X}  =
    \begin{bmatrix}
        a_{0,0} & a_{0,1} & a_{0,2} \\
        a_{1,0} & a_{1,1} & a_{1,2} \\
        a_{2,0} & a_{2,1} & a_{2,2}
    \end{bmatrix}.
\end{equation*}
 Applying this transformation to each vertex of a virtual 3D object results in a rotation of the whole object around the origin $(0,0,0)$ with the equation\\
\begin{equation*}
    \begin{pmatrix}
        x' \\
        y' \\
        z'
    \end{pmatrix}
    =
    \begin{bmatrix}
        a_{0,0} & a_{0,1} & a_{0,2} \\
        a_{1,0} & a_{1,1} & a_{1,2} \\
        a_{2,0} & a_{2,1} & a_{2,2}
    \end{bmatrix}
    \cdot
    \begin{pmatrix}
        x \\
        y \\
        z
    \end{pmatrix}.
\end{equation*}
Moving an object in space without any rotation is generally done with a vector addition, creating an inhomogenous linear equation. Adding a fourth dimension allows packing the spatial translation into the matrix. By extending the vectors with a $1$ and using the fourth column in the matrix to alter $X$, $Y$ and $Z$, these vector entries can be moved without applying any rotation with the equation
\begin{equation*}
    \begin{pmatrix}
        x+\Delta X \\
        y+\Delta Y \\
        z+\Delta Z \\
        1
    \end{pmatrix}
    =
    \begin{pmatrix}
        x' \\
        y' \\
        z' \\
        1
    \end{pmatrix}
    =
    \begin{bmatrix}
        1 & 0 & 0 & \Delta X \\
        0 & 1 & 0 & \Delta Y \\
        0 & 0 & 1 & \Delta Z \\
        0 & 0 & 0 & 1
    \end{bmatrix}
    \cdot
    \begin{pmatrix}
        x \\
        y \\
        z \\
        1
    \end{pmatrix}.
\end{equation*}
To combine the rotation matrix with the translation matrix, the 3x3 rotation matrix gets placed top-left into the 4x4 unit matrix. Now, the rotation matrix also being a 4x4 matrix, rotations and translations can be chained up following the common laws of linear algebra. Chaining up translations and rotations allows moving the rotation axis for an object.
\begin{equation*}
    \begin{pmatrix}
        x' \\
        y' \\
        z' \\
        1
    \end{pmatrix}
    =
    \begin{bmatrix}
        a_{0,0} & a_{0,1} & a_{0,2} & \Delta X \\
        a_{1,0} & a_{1,1} & a_{1,2} & \Delta Y \\
        a_{2,0} & a_{2,1} & a_{2,2} & \Delta Z \\
        0       & 0       & 0       & 1
    \end{bmatrix}
    \cdot
    \begin{pmatrix}
        x \\
        y \\
        z \\
        1
    \end{pmatrix}
\end{equation*}
The dependency on the order of the rotations poses a problem visualized in Figure \ref{im:EulerRotation}: The values returned by a gyroscope need to be applied all at once and not one after another. Replacing rotation matrices by quaternions, described in Section \ref{sec:RotationQuaternion}, solves this problem.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/euler_rotation.png}
    \caption{Euler rotations are dependent on the order of the individual rotations. Rotating around X and then Z results in a different outcome, than first rotating around Z and then around X.}
    \label{im:EulerRotation}
\end{figure}
\subsection{Rotation with Quaternions}
\label{sec:RotationQuaternion}
Quaternions - also known as "Hamilton Numbers" - are the four dimensional equivalent to complex numbers. Analogue to complex numbers, quaternions also consist of a real part, but add three imaginary parts $\textbf{i}$, $\textbf{j}$ and $\textbf{k}$. Most often, quaternions get represented in the form $a+b\textbf{i}+c\textbf{j}+d\textbf{k}$ or a four dimensional vector $(a, b, c, d)$.\\
The relations of the different imaginary parts in a quaternion are defined as $\textbf{i}^{2}=\textbf{j}^{2}=\textbf{k}^{2}=\textbf{ijk}=-1$, $\textbf{ij}=\textbf{k}, \textbf{jk}=\textbf{j}, \textbf{ki}=\textbf{j}$, and  $\textbf{ji}=-\textbf{k}, \textbf{kj}=-\textbf{i}, \textbf{ik}=-\textbf{j}$. With these definitions, the quaternion multiplication, also named Hamilton Product, is non-commutative. The Hamilton Product is
\begin{equation*}
    (a_{1}+b_{1}\textbf{i}+c_{1}\textbf{j}+d_{1}\textbf{k})(a_{2}+b_{2}\textbf{i}+c_{2}\textbf{j}+d_{2}\textbf{k})
    =
    \begin{pmatrix}
          & a_1a_2 &  & - b_1b_2 &  & - c_1c_2 &  & - d_1d_2             \\
        ( & a_1b_2 &  & + b_1a_2 &  & + c_1d_2 &  & - d_1c_2) \textbf{i} \\
        ( & a_1c_2 &  & - b_1d_2 &  & + c_1a_2 &  & + d_1b_2) \textbf{j} \\
        ( & a_1d_2 &  & + b_1c_2 &  & - c_1b_2 &  & + d_1a_2) \textbf{k}
    \end{pmatrix}.
\end{equation*}
The neutral element, analogue to an identity matrix, of the quaternion multiplication is:
\begin{equation*}
    \begin{pmatrix}
        1           \\
        0\textbf{i} \\
        0\textbf{j} \\
        0\textbf{k}
    \end{pmatrix}
\end{equation*}
A three-dimensional vector is written in quaternions by only using the imaginary components.
\begin{equation*}
    \begin{pmatrix}
        x \\
        y \\
        z \\
    \end{pmatrix}
    =
    \begin{pmatrix}
        0           \\
        x\textbf{i} \\
        y\textbf{j} \\
        z\textbf{k}
    \end{pmatrix}
\end{equation*}
According to Euler's rotation theorem, providing a certain angle $\theta$ and a rotation axis $\vec{v_{r}}$, allows describing any rotation in three-dimensional space.
These values - the angle and the axis - are embedded within the rotation quaternion. In addition, the rotation quaternion is required to have the norm being equal one, analogue to a rotation matrix' determinant. Such a rotation quaternion is sometimes named a unit quaternion or a versor.
\begin{equation*}
    \vec{v}_{r} =
    \begin{pmatrix}
        x \\
        y \\
        z \\
    \end{pmatrix}
    \quad ; \quad
    \vec{v}_{r,norm} =
    \begin{pmatrix}
        \frac{x}{\lVert v_{r} \rVert} \\
        \frac{y}{\lVert v_{r} \rVert} \\
        \frac{z}{\lVert v_{r} \rVert}
    \end{pmatrix} =
    \begin{pmatrix}
        x_{norm} \\
        y_{norm} \\
        z_{norm}
    \end{pmatrix}
    \quad ; \quad
    Q_{rot}=
    \begin{pmatrix}
        \cos (\frac{\theta}{2} )                 \\
        x_{norm}\sin(\frac{\theta}{2})\textbf{i} \\
        y_{norm}\sin(\frac{\theta}{2})\textbf{j} \\
        z_{norm}\sin(\frac{\theta}{2})\textbf{k}
    \end{pmatrix}
    \quad ; \quad
    \lVert Q_{Rot} \rVert = 1
\end{equation*}
To apply this rotation quaternion $Q_{rot}$ to a vector $\vec{v}$, it needs to be multiplied from the left and conjugated and multiplied from the right. Conjugation of a unit quaternion is done by flipping the sign in each imaginary part. The result 
\begin{equation*}
    \vec{u} = Q_{rot}\vec{v}Q_{rot}^{-1}=\begin{pmatrix}
        \cos (\frac{\theta}{2} )                 \\
        x_{norm}\sin(\frac{\theta}{2})\textbf{i} \\
        y_{norm}\sin(\frac{\theta}{2})\textbf{j} \\
        z_{norm}\sin(\frac{\theta}{2})\textbf{k}
    \end{pmatrix}
    \begin{pmatrix}
        0           \\
        a\textbf{i} \\
        b\textbf{j} \\
        c\textbf{k}
    \end{pmatrix}
    \begin{pmatrix}
        \cos (\frac{\theta}{2} )                  \\
        -x_{norm}\sin(\frac{\theta}{2})\textbf{i} \\
        -y_{norm}\sin(\frac{\theta}{2})\textbf{j} \\
        -z_{norm}\sin(\frac{\theta}{2})\textbf{k}
    \end{pmatrix}
\end{equation*}
is the rotated vector
\begin{equation*}
\vec{v} =
\begin{pmatrix}
    a \\
    b \\
    c \\
\end{pmatrix}
=
\begin{pmatrix}
    0           \\
    a\textbf{i} \\
    b\textbf{j} \\
    c\textbf{k}
\end{pmatrix}.
\end{equation*}
By converting a rotation quaternion to a rotation matrix, the gap to the standard linear algebra can be bridged. This formula only applies to true rotation quaternions, therefore the norm must be equal to one. 
\begin{equation*}
    Q_{Rot}=    \begin{pmatrix}
        a           \\
        b\textbf{i} \\
        c\textbf{j} \\
        d\textbf{k}
    \end{pmatrix}
    \quad ; \quad
    \lVert Q_{Rot} \rVert = 1
    \quad ; \quad
    A_{Rot}
    =
    \begin{bmatrix}
        1-2(c^{2}+d^{2}) & 2(bc-ad) & 2(bd+ac) \\
        2(bc+ad) & 1-2(b^{2}+d^{2}) & 2(cd-ab) \\
        2(bd-ac) & 2(cd+ab) & 1-2(b^{2}+c^{2})
    \end{bmatrix}
\end{equation*}
The basic quaternion operations - like the multiplication - are included in the linmath-library. The linmath-library represents a quaternion as a four-dimensional float-vector in the ordering $(b, c, d, a)$ - the real part being the last element.\\

\subsection{Singular Value Decomposition (SVD)}
\label{sec:SVD}
Any matrix multiplication is a combination of rotations and stretching. The singular Value Decomposition allows splitting any matrix into three fundamental transformations, either pure rotations, reflections or pure stretchings. The SVD is a required step in finding the rigid motion of a point cloud\cite{SVD_ETH}.\\
The SVD decomposes any given matrix $M$ of the dimension $n\times m$ into three matrices $U$ of dimension $n\times n$, $\Sigma$ of dimension $n\times m$, and $V$ of dimension $m\times m$.\cite{SVD_MIT} If $M$ is real, $U$ and $V$ are guaranteed to be orthogonal, while $\Sigma$ is a diagonal matrix. The matrices fulfill the following equation: 
\begin{equation*}
    M= U\Sigma V^{T}
\end{equation*}
The individual matrices generated by the SVD act als two rotations and one distortion. As visible in Figure \ref{im:SVD} on the next page, the unit circle $\vec{x}$ first gets rotated by $V^{T}$ that the stretching is in the directions of the coordinate system. After applying the stretching $\Sigma$ to the rotated circle $\vec{y}_{1}$, the intermediate ellipse $\vec{y}_{2}$ gets rotated and reflected to match $\vec{y}$. 
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{images/SVD}
    \caption{Singular Value Decomposition in 2D shown in the individual stages. Blue is before and red is after the applied transformation. On top left, the entire matrix operation is demonstrated. The  figure on top right demonstrates the first rotation and the figure on bottom left shows the stretching. The final reflection of the stretched ellipse is shown on bottom right.}
    \label{im:SVD}
\end{figure}
The following recipe\cite{SVD_MIT} shows how to find the matrices of the singular value decomposition. The matrix 
\begin{equation*}
    M= 
    \begin{bmatrix}
        0.6 & 0.9  \\
        1.1 & 0.2
    \end{bmatrix}  
\end{equation*}
that is used in Figure \ref{im:SVD} gets decomposed.
In order to find $U$, the eigenvectors $\vec{v}_{1}$ and $\vec{v}_{2}$ of $MM^{T}$ have to be calculated, that are directly filled in:
\begin{equation*}
    MM^{T}= 
    \begin{bmatrix}
        1.17 & 0.84  \\
        0.84 & 1.25
    \end{bmatrix}  \quad
    \vec{v}_{1} =
    \begin{pmatrix}
        -0.6901 \\
        -0.7237
    \end{pmatrix}\quad
    \vec{v}_{2} =
    \begin{pmatrix}
        -0.7237 \\
        0.6901
    \end{pmatrix}\quad
    U= 
    \begin{bmatrix}
        -0.6901 & -0.7237  \\
        -0.7237 & 0.6901
    \end{bmatrix}
\end{equation*}
Similarly to $U$, the eigenvectors of $M^{T}M$ deliver $V$:
\begin{equation*}
    M^{T}M= 
    \begin{bmatrix}
        1.57 & 0.76  \\
        0.76 & 0.85
    \end{bmatrix}  \quad
    \vec{v}_{1} =
    \begin{pmatrix}
        -0.8450 \\
        0.5347
    \end{pmatrix}\quad
    \vec{v}_{2} =
    \begin{pmatrix}
        -0.5347 \\
        -0.8450
    \end{pmatrix}\quad
    V= 
    \begin{bmatrix}
        -0.8450 & -0.5347  \\
        0.5347 & -0.8450
    \end{bmatrix}
\end{equation*}
Finally, the square-roots of the eigenvalues of either $M^{T}M$ or $MM^{T}$ are the diagonal values of $\Sigma$:
\begin{equation*}
    \Sigma= 
    \begin{bmatrix}
        1.4321 & 0  \\
        0 & 0.6075
    \end{bmatrix}
\end{equation*}
The determinant of $U$ and $V$ allows checking if they are rotations or reflections. A rotation has a determinant of 1, a reflection a determinant of -1.
\subsection{Spatial coordinates and device coordinates}
\label{sec:ABC_XYZ_coords}
As the rotation of the camera head changes the coordinates of the measurement in respect to real-world coordinates, a convention helps carry out the calculations. For the sensors on the camera head, the coordinates are named $a$, $b$, and $c$, while the spatial coordinates are named $x$, $y$, and $z$.
The coordinates can be transformed by the following formula, knowing the current orientation quaternion $Q_{rot}$ of the camera.   
\begin{equation*}
    Q_{rot}
    =
    \begin{pmatrix}
        r           \\
        u\textbf{j} \\
        v\textbf{j} \\
        w\textbf{k}
    \end{pmatrix}
    \quad ; \quad
    \begin{pmatrix}
        0           \\
        x\textbf{i} \\
        y\textbf{j} \\
        z\textbf{k}
    \end{pmatrix}
    =
    \begin{pmatrix}
        r           \\
        u\textbf{i} \\
        v\textbf{j} \\
        w\textbf{k}
    \end{pmatrix}
    \begin{pmatrix}
        0           \\
        a\textbf{i} \\
        b\textbf{j} \\
        c\textbf{k}
    \end{pmatrix}
    \begin{pmatrix}
        r           \\
        -u\textbf{i} \\
        -v\textbf{j} \\
        -w\textbf{k}
    \end{pmatrix}
\end{equation*}
Section \ref{sec:RotationQuaternion} describes how to carry out the mathematics of this quaternion multiplication. 
\section{Standard Vulkan Coordinate System}
\label{sec:VulkanCoords}
In Vulkan, every vertex coordinate of a 3D rendered object gets mapped to the nearest pixel in the viewport window. This vertex mapping is done in multiple steps from local space coordinates via clip coordinates towards normalized device coordinates to the pixel coordinates.
A 3D object is a group of vertex coordinates, described by a list of three-dimensional vectors $\vec{v_{v}} = (x_{v},y_{v},z_{v})$. The subscript $v$ denotes the vertex coordinates which reside in the object's local space. These coordinates usually do not contain data regarding the whole object's scale, position, and rotation in 3D space.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.52\textwidth]{images/Vulkan_local_space.eps}
    \caption{A Vulkan object consisting of four vertices in its local space. The green corner acts as an example in the ongoing section.}
    \label{im:VulkanLocalSpace}
\end{figure}
Vulkan expects the output of the shader step to be in clip coordinates. The clip coordinates, denoted with the subscript $c$, reside in the clip space. Clip coordinates are four-dimensional vectors $\vec{v_{c}} = (x_{c},y_{c},z_{c},w_{c})$ and the result of a matrix multiplication operation.\\
\begin{equation*}
    \vec{v_{c}} = A \cdot  \vec{v_{v}}
\end{equation*}
\begin{equation*}
    \begin{pmatrix}
        x_{c} \\
        y_{c} \\
        z_{c} \\
        w_{c}
    \end{pmatrix}
    =
    \begin{bmatrix}
        a_{0,0} & a_{0,1} & a_{M 0,2} & a_{0,3} \\
        a_{1,0} & a_{1,1} & a_{M 1,2} & a_{1,3} \\
        a_{2,0} & a_{2,1} & a_{M 2,2} & a_{2,3} \\
        a_{3,0} & a_{3,1} & a_{M 3,2} & a_{3,3}
    \end{bmatrix}
    \cdot
    \begin{pmatrix}
        x_{v} \\
        y_{v} \\
        z_{v} \\
        1
    \end{pmatrix}
\end{equation*}
Generally, three combined matrix multiplications describe how a 3D object is rendered – the model matrix, the view matrix, and an added projection matrix. The model matrix $A_{Model}$ defines the scale, rotation and position of the 3D object in the world space. The world space is the coordinate system in which multiple objects and virtual cameras get placed, to assemble the scene. The model matrix is a standard 4x4 rotation and translation matrix as explained in Section \ref{sec:LinAlgRotation}. In the example, shown in the Figures \ref{im:VulkanLocalSpace} and \ref{im:VulkanCoords}, the rectangle is shifted on the x-axis.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/Vulkan_coordinates.eps}
    \caption{The rectangle is placed with the model-matrix, while the view- and projection-matrices determine the perspective. The green corner point acts as the example. Note the inverted y-axis in clip space.}
    \label{im:VulkanCoords}
\end{figure}

\begin{equation*}
    A_{Model} =
\begin{bmatrix}
    1 & 0 & 0 & 2 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
\end{bmatrix}
\end{equation*}
The view matrix $A_{View}$ is also a rotation and translation matrix, but describes the position and direction of the viewport camera inside the world space. Rotating the camera rotates the rendered virtual space, which indirectly moves and turns the models in the viewport. The linmath-library\cite{linmath_lib} offers the "4x4\_look\_at" function to calculate the view matrix based on a virtual camera at a specific position position, a viewing-, and an upwards-direction, as visualized in red in Figure \ref{im:VulkanCoords}. In the upper half of the figure, the virtual camera is placed at the source, pointing towards the $x$-axis with the upwards direction facing alongside $z$. In the bottom part of the figure, the camera position is shifted, and the camera is rotated slightly.
\begin{equation*}
    A_{View, top} = 
\begin{bmatrix}
    0 & -1& 0 & 0 \\
    0 & 0 & 1 & 0 \\
    -1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1
\end{bmatrix} \quad ; \quad
    A_{View, bottom} =
\begin{bmatrix}
    0.266 &-0.963 & -0.036 & 0.036 \\
    0.266 & 0.037 & 0.963 & -0.963 \\
    -0.927 &-0.266 & 0.266 & -0.266 \\
    0 & 0 & 0 & 1
\end{bmatrix}
\end{equation*}
The projection matrix $A_{Projection}$ reduces the vertex' 3d coordinates to the viewport plane by projecting them onto a virtual screen. The linmath-library offers the "4x4\_perspective" function that calculates the projection matrix based on a given field of view angle. As the linmath-library was made for OpenGL, which uses a different alignment on the framebuffer, the element $A_{Projection}[1][1]$ needs to be multiplied by $-1$. In Figure \ref{im:VulkanCoords}, the effect of the projection matrix is shown with the black beams protruding out of the red dot. In the example, the following projection matrix is used.
\begin{equation*}
    A_{Projection} =
\begin{bmatrix}
    0.678 & 0 & 0 & 0 \\
    0 & -1.19175 & 0 & 0 \\
    0 & 0 & -1.0202 & -0.20202 \\
    0 & 0 & 1 & 0
\end{bmatrix}
\end{equation*}
Within the vertex shader, these three matrices are chained to perform the desired transformation
\begin{equation*}
    A = A_{Projection} \cdot A_{View} \cdot A_{Model}.
\end{equation*}
Applying the example matrices to the green dot demonstrates the process of calculating the clip coordinates $\vec{v_{c}}$. The developer has to provide these coordinates to Vulkan as the output of the vertex shader:
\begin{equation*}
    \vec{v_{v}} =     
    \begin{pmatrix}
        0 \\
        1 \\
        0.5625 \\
        1
    \end{pmatrix} \quad ; \quad
    \vec{v_{c, top}} = A_{top} \cdot  \vec{v_{v}} =     
    \begin{pmatrix}
        -0.678 \\
        -0.670 \\
        1.838 \\
        2.0
    \end{pmatrix}
    \quad ; \quad
    \vec{v_{c, bottom}} = A_{bottom} \cdot  \vec{v_{v}} =     
    \begin{pmatrix}
        -0.281 \\
        -0.175 \\
        2.079 \\
        2.235
    \end{pmatrix}
\end{equation*}
By division of the clip coordinate components $x_{c}$, $y_{c}$ and $z_{c}$ by $w_{c}$, Vulkan itself calculates the normalized device coordinates $\vec{v_{NDC}} = (x_{NDC}, y_{NDC}, z_{NDC})$.
\begin{equation*}
    \begin{pmatrix}
        x_{NDC} \\
        y_{NDC} \\
        z_{NDC} \\
    \end{pmatrix}
    =
    \begin{pmatrix}
        \frac{x_{c}}{w_{c}} \\
        \frac{y_{c}}{w_{c}} \\
        \frac{z_{c}}{w_{c}} \\
    \end{pmatrix}
\end{equation*}
In the example, the normalized device coordinates are: 
\begin{equation*}
    \vec{v_{NDC,top}} =     
    \begin{pmatrix}
        -0.339 \\
        -0.335 \\
        0.919
    \end{pmatrix} \quad ; \quad
    \vec{v_{NDC,bottom}} = 
    \begin{pmatrix}
        -0.126 \\
        -0.078 \\
        0.930
    \end{pmatrix}
\end{equation*}
The transformation to the pixel coordinates are also done by Vulkan without requiring any action by the developer. Only the $x$ and $y$ parts of the normalized device coordinates define, where a pixel is rendered. 
 The $z$ part tells the depth, on how far the vertex is inside the monitor. Video games use this information to not render objects that are too far away from the observer. On the viewport surface, the point $(0 / 0)$ is located in the center. Top left is $(-1 / -1)$, top right is $(1 / -1)$, bottom left is $(-1 / 1)$ and bottom right is $(1 / 1)$.\\
If a vertex falls outside of the range ±1 in the clip coordinates, it is not rendered, therefore clipped away, hence the name of the coordinate space.
\newpage
\section{Camera Calibration}
\label{sec:FundCamCalibration}
An uncalibrated camera image often has lens distortion, warping a rectangle into a pillow or barrel shape and making areas appear closer in certain parts of the image. These distortions are named radial and tangential distortion and are induced by the camera lens.\\
According to OpenCV \cite{openCVCamCalib}, radial distiortion is modeled as
\begin{equation*}
x_{distorted} = x(1+k_{1}r^{2}+k_{2}r^{4}+k_{3}r^{6})\quad,\quad
y_{distorted} = y(1+k_{1}r^{2}+k_{2}r^{4}+k_{3}r^{6}).
\end{equation*}
Similarly, tangential distortion is modeled as
\begin{equation*}
    x_{distorted} = x+(2p_{1}xy+p_{2}(r^{2}+2x^{2}))\quad,\quad
    y_{distorted} = y+(p_{1}(r^{2}+2y^{2})+2p_{2}xy).
\end{equation*}
In these equations, $r$ is the euclidian distance between the distorted image point and the distortion center.\\
\begin{equation*}
    r=\sqrt{(x_{distorted}-x_{center})^{2}+(y_{distorted}-y_{center})^{2}}
\end{equation*}
Therefore, to compensate for the lens distortion, the five coefficients $k_{1}$, $k_{2}$, $k_{3}$, $p_{1}$ and $p_{2}$ need to be estimated by calibration. In addition, the effect of the focal length $f$ and the optical center $c$ get expressed as a 3x3 matrix.
\begin{equation*}
    A_{Camera}=
    \begin{bmatrix}
        f_{x} & 0 & c_{x} \\
        0 & f_{y} & c_{y} \\
        0 & 0 & 1
    \end{bmatrix}
\end{equation*}
OpenCV itself provides a script which estimates these values based on multiple photographs of chess boards. Applying these corrections leads to a smaller image as parts near the border get cut off.
\begin{figure}[H]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
      \includegraphics[scale=0.70]{images/camcalib_source.jpg}
      \subcaption{Before correction}
    \end{minipage} % Hier darf keine Leerzeile zwischen den beiden Minipages liegen!
    \begin{minipage}[b]{0.45\textwidth}
      \includegraphics[scale=0.70]{images/camcalib_result.png} 
      \subcaption{After correction}
    \end{minipage}
    \caption{Camera correction demonstrated at the grayscale image of the ToF camera.}
    \label{fig.camCalib}
  \end{figure}
The implementation in CUDA stores the pixel coordinates of the uncorrected image for each pixel in the corrected image. This data is loaded into a CUDA allocated memory area and provides a direct coordinate mapping with lookup tables as shown in Figure \ref{im:CudaCamCalib}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/undistort_LUT.pdf}
    \caption{Applying the lens correction with the use of lookup tables (LUT) in CUDA.}
    \label{im:CudaCamCalib}
\end{figure}

\section{Kalman filter}
\label{sec:Kalmanfilter}
The Kalman filter is an adaptive filter realized with a predictor-corrector algorithm that uses a model and known gaussian uncertainties of different sensors to estimate a value. Additionally to sensor data, the Kalman filter can adjust the estimate based on known forces, for example an attached motor.\\
In this thesis, as the camera head will be moving freely, motion without a known input needs to be measured, having sensors for acceleration, velocity, and rotation. The following example demonstrates the position $p$, velocity $v$, and acceleration $a$ in one dimension. In Figure \ref{im:KalmanFilter1D} a fifth-order spline is used for the position data from which velocity and acceleration are derived to simulate the sensor data. Adding Gaussian noise to the simulated velocity and acceleration data reflects reality and allows demonstrating the Kalman filter.\\
For each new data point, the Kalman filter predicts the next system state based on old data using the model matrix $F$. If a known input $\vec{u}$ would act on the system, the input matrix $B$ models that. Without a known input, $B$ and $\vec{u}$ are omitted and the system is entirely model based. The system state $\vec{x}$ transitions from the prior state $k-1$ to the prediction $k|k-1$. 
\begin{equation*}
    \vec{x}_{k|k-1} = 
    F
    \cdot
    \vec{x}_{k-1}
    (+
    B
    \cdot
    \vec{u}) \qquad ; \qquad
    \begin{pmatrix}
        p  \\
        v  \\
        a 
    \end{pmatrix}_{k|k-1} = 
    \begin{bmatrix}
        1 & \Delta t & \frac{\Delta t^{2}}{2} \\
        0 & 1 & \Delta t \\
        0 & 0 & 1
    \end{bmatrix}
    \cdot
    \begin{pmatrix}
        p  \\
        v  \\
        a 
    \end{pmatrix}_{k-1}    
\end{equation*}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/Kalman_example_1d.eps}
    \caption{The Kalman filter demonstrated. Blue is the acceleration, purple the velocity and red the position. Raw integration of the velocity, on top right, shows a jagged output of the position estimation. Raw double-integration of the acceleration, on bottom left, leads to the position to drift away. The position estimate of the Kalman filter, on bottom right, itself follows the jagged line of the raw velocity integration, but the estimated velocity is closer to the true value. In green, the integration of the estimated velocity shows a better approximation of the true value.}
    \label{im:KalmanFilter1D}
\end{figure}
In the example, the system matrix $F$ reflects the derivations of the position $p$. The first time-derivative of $p$ is the velocity $v$ and the second time-derivative is the acceleration $a$.\\
The evolution of the covariance matrix of the errors is :
\begin{equation*}
    P_{k|k-1} = 
    F
    \cdot
    P_{k-1}
    \cdot
    F^{T}
    +
    N
\end{equation*}
with the gaussian process noise $N$. Assuming that an external force $\vec{f}$ acts on the system, which is constant during the sampling intervals $\Delta t$. The external force influences the acceleration, the velocity, and ultimately the position proportional to the input vector
\begin{equation*}
     G = 
\begin{pmatrix}
    \frac{\Delta t^{2}}{2}  \\
    \Delta t  \\
    1 
\end{pmatrix}.\\ 
\end{equation*}
The process noise matrix $N$ combines the input vector $G$ and the standard deviation of the external force $\sigma_{f}$. The standard deviation of the external force $\sigma_{f}$ is set to 1, as expected accelerations are much smaller than the gravitational acceleration. The used process noise model is the piecewise white noise model.
\begin{equation*}
    N = G\cdot G^{T}\cdot \sigma_{f} =
    \begin{bmatrix}
        \frac{\Delta t^{4}}{4} & \frac{\Delta t^{3}}{2} & \frac{\Delta t^{2}}{2} \\
        \frac{\Delta t^{3}}{2} & \Delta t^{2} & \Delta t \\
        \frac{\Delta t^{2}}{2} & \Delta t & 1
    \end{bmatrix}
    \cdot 1
\end{equation*}
The covariance matrix of the errors $P$ shows that the position estimate will worsen over time. The error propagation causes this error to steadily increase without receiving a direct correction from any sensor. The system needs the possibility to determine the position to avoid long-term drift; otherwise, the error increases over time. On the other values - where sensor values are present - the covariance matrix of errors $P$ converges towards a constant error.
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/Kalman_example_1d_error.eps}
    \caption{On the left, the diagonal values of the covariance matrix of the errors are visualized against each other. The errors for velocity and acceleration converge towards a low value, while the error for the position increases.\\
    On the right, a close-up of the internal workings of the Kalman filter. The blue dots are the raw measurements, the red dots are the corresponding predictions and plotted in green is the output value of the Kalman filter. In contrast, the true velocity is plotted in grey.}
    \label{im:KalmanError}
\end{figure}
With the system state $\vec{x}_{k|k-1}=F\cdot\vec{x}_{k-1}$ and the covariance matrix $P_{k|k-1}=F\cdot P_{k-1}\cdot F^{T}+N$ being predicted, the following steps describe the correction part of the Kalman iteration. The observation matrix $H$ determines the sensor influence on the system state vector $\vec{x}$. For each correction step, a Kalman Gain matrix $K_{k}$ determines the optimal correction based on the current covariance of errors $P$ and the measurement noise $R$.
\begin{equation*}
    K_{k} = P_{k|k-1}\cdot H^{T}(H\cdot P_{k|k-1}\cdot H^{T}+R)^{-1}
    \qquad ; \qquad
    H=
    \begin{bmatrix}
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}
\end{equation*}
$H$ is a $2\times 3$ matrix, because there are only two measurements versus the three dimensional system state. Therfore, the matrix inversion for the bracket in the formula above is only of dimension $2\times 2$.\\
With the Kalman gain present, the system is ready to correct both the system state vector $\vec{x}$ and the covariance matrix of errors $P$. The vector $\vec{z}_{k}$ contains the new observation.
\begin{equation*}
    x_{k} = \vec{x}_{k|k-1}+K_{k}(\vec{z}_{k}-H\cdot \vec{x}_{k|k-1})
    \qquad ; \qquad
    P_{k}= (I-K_{k}\cdot H)P_{k|k-1}
\end{equation*}
For multiple sensors influencing the same entry of the system state vector $\vec{x}$, multiple correction steps may be chained after each other.